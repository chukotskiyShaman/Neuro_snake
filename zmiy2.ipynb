{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccbce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from numpy import unravel_index as unravel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import heapq\n",
    "import math\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0267406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ad760d-ccb6-42b2-ae9f-cfb61245c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = {'a': torch.tensor([0., -1.]), 'd': torch.tensor([0., 1.]), 'w': torch.tensor([-1., 0.]), 's': torch.tensor([1., 0.])}\n",
    "\n",
    "def do(snake: torch.Tensor, action):\n",
    "    reward = 0\n",
    "    positions = snake.flatten().topk(2)[1]\n",
    "    [pos_cur, pos_prev] = [torch.Tensor(unravel(x, snake.shape)) for x in positions]\n",
    "    #print('direction', (pos_cur - pos_prev)) # Направление движения\n",
    "    pos_next = (pos_cur + action) % torch.Tensor([snake.shape]).squeeze(0) \n",
    "    \n",
    "    pos_cur = pos_cur.int()\n",
    "    pos_next = pos_next.int()\n",
    "    \n",
    "    # Проверка на столкновение\n",
    "    if (snake[tuple(pos_next)] > 0).any():\n",
    "        reward = -100\n",
    "        return reward,(snake[tuple(pos_cur)] - 2).item()  # Возвращаем счёт (длина змейки минус 2)\n",
    "    \n",
    "    # Кушаем яблоко\n",
    "    if snake[tuple(pos_next)] == -1:\n",
    "        pos_food = (snake == 0).flatten().to(torch.float).multinomial(1)[0] # Генерируем позицию яблока\n",
    "        snake[unravel(pos_food, snake.shape)] = -1 # Добавляем яблоко в игру\n",
    "        reward= 10\n",
    "        \n",
    "    else: # Двигаемся в пустую клетку\n",
    "        snake[snake > 0] -= 1  # Устанавливаем все значения в теле змеи равными 1\n",
    "\n",
    "    snake[tuple(pos_next)] = snake[tuple(pos_cur)] + 1 # перемещаем голову\n",
    "    return reward, (snake[tuple(pos_cur)] - 2).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1bb55ee-66a2-42ee-8758-0018f9ffb796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuro_BigBoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.conv1=nn.Conv2d(3, 32, kernel_size=(3,3), padding = 1)\n",
    "        # self.conv1=nn.Conv2d(32, 64, kernel_size=(3,3), padding = 1)\n",
    "        # self.fl = nn.Flatten()\n",
    "        self.fc1=nn.Linear(32*32, 256)\n",
    "        self.fc2=nn.Linear(256,3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        # x= F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ccee6c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = torch.optim.SGD(model.parameters(), lr=self.lr, momentum=0.4, weight_decay=0.0001)\n",
    "        self.criterion = nn.CrossEntropyLoss()#.cuda()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # print(state)\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        # (n, x)\n",
    "        if len(state.shape) == 2:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "    \n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9e56e07-1c39-4668-9db9-70d2f6f671c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Champion():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_games = 0\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.model = Neuro_BigBoss()#.cuda()\n",
    "        self.eps = 0\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "            # print(1)\n",
    "        else:\n",
    "            # print(2)\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        # states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        # states = list(states)\n",
    "        # # actions = list(actions)\n",
    "        # # rewards = list(rewards)\n",
    "        # next_states = list(next_states)\n",
    "        # dones = list(dones)\n",
    "        # print(states)\n",
    "        # self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "        for state, action, reward, next_state, done in mini_sample:\n",
    "           self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            prediction = self.model(state)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4ad2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x  # Координата x узла на карте\n",
    "        self.y = y  # Координата y узла на карте\n",
    "        self.g = 0  # Расстояние от начального узла до текущего узла\n",
    "        self.h = 0  # Примерное расстояние от текущего узла до конечного узла\n",
    "        self.f = 0  # Сумма g и h\n",
    "        self.parent = None  # Родительский узел, используется для восстановления пути\n",
    "\n",
    "    # Переопределяем оператор сравнения для сравнения узлов\n",
    "    def __lt__(self, other):\n",
    "        return self.f < other.f\n",
    "\n",
    "    # Переопределяем оператор равенства для сравнения узлов\n",
    "    def __eq__(self, other):\n",
    "        return self.x == other.x and self.y == other.y\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.x, self.y))\n",
    "\n",
    "# Определяем функцию для нахождения пути с помощью алгоритма A*\n",
    "def astar(start, end, obstacles):\n",
    "\n",
    "    # Создаем начальный и конечный узлы\n",
    "    start_node = Node(start[0], start[1])\n",
    "    end_node = Node(end[0], end[1])\n",
    "\n",
    "    # Инициализируем очередь с приоритетами\n",
    "    open_list = []\n",
    "    heapq.heappush(open_list, start_node)\n",
    "\n",
    "    # Инициализируем множество посещенных узлов\n",
    "    closed_set = set()\n",
    "\n",
    "    # Пока очередь с приоритетами не пуста\n",
    "    while open_list:\n",
    "        # Извлекаем узел с наименьшей оценкой f\n",
    "        current_node = heapq.heappop(open_list)\n",
    "        # Если текущий узел является конечным\n",
    "        if current_node == end_node:\n",
    "            # Восстанавливаем путь от конечного узла до начального\n",
    "            path = []\n",
    "            while current_node.parent is not None:\n",
    "                path.append((current_node.x, current_node.y))\n",
    "                current_node = current_node.parent\n",
    "            return path[-1]\n",
    "\n",
    "        # Добавляем текущий узел в множество посещенных узлов\n",
    "        closed_set.add(current_node)\n",
    "\n",
    "        # Получаем соседние узлы\n",
    "        neighbors = []\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                # Игнорируем текущий узел\n",
    "                if not ((dx == -1 and dy == 0) or (dx == 0 and dy == 1) or (dx == 1 and dy == 0) or (dx == 0 and dy == -1)):\n",
    "                    continue\n",
    "                # Вычисляем координаты соседнего узла\n",
    "                x = current_node.x + dx\n",
    "                y = current_node.y + dy\n",
    "                # Игнорируем узлы за пределами карты\n",
    "                if x < 0 :\n",
    "                    x=len(obstacles)-1\n",
    "                if x>=len(obstacles):\n",
    "                    x=0\n",
    "                if y<0:\n",
    "                    y=len(obstacles)-1\n",
    "                if y>=len(obstacles):\n",
    "                    y=0\n",
    "                # Игнорируем препятствия\n",
    "                if obstacles[x][y] == 1:\n",
    "                    continue\n",
    "                # Создаем новый узел и добавляем его в список соседей\n",
    "                neighbor = Node(x, y)\n",
    "                neighbors.append(neighbor)\n",
    "\n",
    "        # Для каждого соседнего узла\n",
    "        for neighbor in neighbors:\n",
    "            # Если соседний узел уже был посещен, пропускаем его\n",
    "            if neighbor in closed_set:\n",
    "                continue\n",
    "\n",
    "            # Вычисляем расстояние от начального узла до соседнего узла\n",
    "            new_g = current_node.g + 1\n",
    "\n",
    "            # Если соседний узел уже находится в очереди с приоритетами\n",
    "            if nfo := next((n for n in open_list if n == neighbor), None):\n",
    "                # Если новое расстояние до соседнего узла меньше, чем старое, обновляем значения g, h и f\n",
    "                if new_g < nfo.g:\n",
    "                    nfo.g = new_g\n",
    "                    nfo.h = math.sqrt((end_node.x - nfo.x) ** 2 + (end_node.y - nfo.y) ** 2)\n",
    "                    nfo.f = nfo.g + nfo.h\n",
    "                    nfo.parent = current_node\n",
    "                    # Обновляем приоритет соседнего узла в очереди с приоритетами\n",
    "                    heapq.heapify(open_list)\n",
    "            else:\n",
    "                # Иначе добавляем соседний узел в очередь с приоритетами и вычисляем значения g, h и f\n",
    "                neighbor.g = new_g\n",
    "                neighbor.h = math.sqrt((end_node.x - neighbor.x) ** 2 + (end_node.y - neighbor.y) ** 2)\n",
    "                neighbor.f = neighbor.g + neighbor.h\n",
    "                neighbor.parent = current_node\n",
    "                heapq.heappush(open_list, neighbor)\n",
    "\n",
    "    # Если конечный узел недостижим, возвращаем None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "445a9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Snake:\n",
    "    def __init__(self):\n",
    "        self.field = torch.zeros((32, 32), dtype=torch.float)\n",
    "        self.field[0, :3] = torch.Tensor([1, 2, -1]) # [хвост, голова, яблоко]\n",
    "        self.a = torch.zeros((32, 32), dtype=torch.float)\n",
    "        self.dirrection = torch.tensor([0,1])\n",
    "        self.head_cords = [0,1]\n",
    "        self.apple_cords = [0,2]\n",
    "        self.neighbours = [[31,1],[0,0],[0,2],[1,1]]\n",
    "        self.collision = [False, True, False, False]\n",
    "        self.availible_passes = [torch.tensor([-1,0]),torch.tensor(self.dirrection),torch.tensor([1,0])]\n",
    "    \n",
    "    def set_dirrection(self, dir):\n",
    "        dir = torch.tensor(dir)\n",
    "        if not torch.allclose(dir, self.dirrection):\n",
    "            self.dirrection = dir\n",
    "            if torch.allclose(dir, torch.tensor([0, 1])):\n",
    "                self.availible_passes = [torch.tensor([-1, 0]), torch.tensor(self.dirrection), torch.tensor([1, 0])]\n",
    "            if torch.allclose(dir, torch.tensor([1, 0])):\n",
    "                self.availible_passes = [torch.tensor([0, 1]), torch.tensor(self.dirrection), torch.tensor([0, -1])]\n",
    "            if torch.allclose(dir, torch.tensor([0, -1])):\n",
    "                self.availible_passes = [torch.tensor([1, 0]), torch.tensor(self.dirrection), torch.tensor([-1, 0])]\n",
    "            if torch.allclose(dir, torch.tensor([-1, 0])):\n",
    "                self.availible_passes = [torch.tensor([0, -1]), torch.tensor(self.dirrection), torch.tensor([0, 1])]\n",
    "    \n",
    "    def set_head_cords(self,head):\n",
    "        self.head_cords = head\n",
    "    \n",
    "    def set_apple_cords(self,apple):\n",
    "        self.apple_cords = apple\n",
    "    \n",
    "    def set_neighbours(self):\n",
    "        counter = 0\n",
    "        for dx in range(-1, 2):\n",
    "            for dy in range(-1, 2):\n",
    "                # Игнорируем текущий узел\n",
    "                if not ((dx == -1 and dy == 0) or (dx == 0 and dy == 1) or (dx == 1 and dy == 0) or (dx == 0 and dy == -1)):\n",
    "                    continue\n",
    "                x = self.head_cords[0] + dx\n",
    "                y = self.head_cords[1] + dy\n",
    "\n",
    "                if x < 0 :\n",
    "                    x=31\n",
    "                if x>=32:\n",
    "                    x=0\n",
    "                if y<0:\n",
    "                    y=31\n",
    "                if y>=32:\n",
    "                    y=0\n",
    "\n",
    "                self.neighbours[counter] = [x,y]\n",
    "                if self.field[x][y] > 0:\n",
    "                    self.collision[counter] = True\n",
    "                else:\n",
    "                    self.collision[counter] = False\n",
    "                counter+=1\n",
    "\n",
    "    def get_state(self):\n",
    "        return [self.dirrection,self.head_cords, self.apple_cords,self.neighbours,self.collision]\n",
    "    \n",
    "    def make_step(self, step):\n",
    "        step=self.availible_passes[np.argmax(step)]\n",
    "        reward = 0\n",
    "        a = torch.zeros(self.field.shape)\n",
    "        a[self.field>0]=1\n",
    "        a[self.field==self.field.max()]=2\n",
    "        a[self.field<0]=-1\n",
    "        head = [self.field.topk(1)[0].argmax().numpy(),self.field.topk(1)[1][self.field.topk(1)[0].argmax()].numpy()]\n",
    "        apple = [(self.field.argmin()/32).int().item(),\n",
    "            (self.field.argmin()%32).item()]\n",
    "        head[1]=head[1][0]\n",
    "        head_cords = [head[0].item(),head[1]]\n",
    "        self.set_apple_cords(apple)\n",
    "        self.set_head_cords(head_cords)\n",
    "        self.set_neighbours()\n",
    "        self.set_dirrection(step)\n",
    "        reward, score = do(self.field, step)\n",
    "        done = False\n",
    "        if reward == -100:\n",
    "            done = True\n",
    "        return reward, done, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15d11673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.availible_passes = [torch.tensor([-1,0]),torch.tensor(self.dirrection),torch.tensor([1,0])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dir = torch.tensor(dir)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.availible_passes = [torch.tensor([0, -1]), torch.tensor(self.dirrection), torch.tensor([0, 1])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\3236007800.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_state = torch.tensor(next_state, dtype=torch.float)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.availible_passes = [torch.tensor([-1, 0]), torch.tensor(self.dirrection), torch.tensor([1, 0])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.availible_passes = [torch.tensor([0, 1]), torch.tensor(self.dirrection), torch.tensor([0, -1])]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11488\\254939064.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.availible_passes = [torch.tensor([1, 0]), torch.tensor(self.dirrection), torch.tensor([-1, 0])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 2.0 Record: 2.0\n",
      "Game 2 Score 2.0 Record: 2.0\n",
      "Game 3 Score 2.0 Record: 2.0\n",
      "Game 4 Score 2.0 Record: 2.0\n",
      "Game 5 Score 3.0 Record: 3.0\n",
      "Game 6 Score 2.0 Record: 3.0\n",
      "Game 7 Score 2.0 Record: 3.0\n",
      "Game 8 Score 2.0 Record: 3.0\n",
      "Game 9 Score 2.0 Record: 3.0\n",
      "Game 10 Score 2.0 Record: 3.0\n",
      "Game 11 Score 2.0 Record: 3.0\n",
      "Game 12 Score 2.0 Record: 3.0\n",
      "Game 13 Score 2.0 Record: 3.0\n",
      "Game 14 Score 2.0 Record: 3.0\n",
      "Game 15 Score 2.0 Record: 3.0\n",
      "Game 16 Score 2.0 Record: 3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plot_scores = []\n",
    "plot_mean_scores = []\n",
    "total_score = 0\n",
    "record = 0\n",
    "agent = Champion()\n",
    "game = Snake()\n",
    "while True:\n",
    "    # get old state\n",
    "    state_old = game.field\n",
    "\n",
    "    # get move\n",
    "    final_move = agent.get_action(state_old)\n",
    "\n",
    "    # perform move and get new state\n",
    "    reward, done, score = game.make_step(final_move)\n",
    "\n",
    "    state_new = game.field\n",
    "\n",
    "    # train short memory\n",
    "    agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "    # remember\n",
    "    agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "    if done:\n",
    "        # train long memory, plot result\n",
    "        game=Snake()\n",
    "        agent.n_games += 1\n",
    "        agent.train_long_memory()\n",
    "\n",
    "        if score > record:\n",
    "            record = score\n",
    "            agent.model.save()\n",
    "\n",
    "        print('Game', agent.n_games, 'Score', score, 'Record:', record)\n",
    "\n",
    "        plot_scores.append(score)\n",
    "        total_score += score\n",
    "        mean_score = total_score / agent.n_games\n",
    "        plot_mean_scores.append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e733d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snake = Snake()\n",
    "reward = 0\n",
    "while True:\n",
    "    a = torch.zeros(snake.field.shape)\n",
    "    a[snake.field>0]=1\n",
    "    a[snake.field==snake.field.max()]=2\n",
    "    a[snake.field<0]=-1\n",
    "    head = [snake.field.topk(1)[0].argmax().numpy(),snake.field.topk(1)[1][snake.field.topk(1)[0].argmax()].numpy()]\n",
    "    apple = [(snake.field.argmin()/32).int().item(),\n",
    "        (snake.field.argmin()%32).item()]\n",
    "    head[1]=head[1][0]\n",
    "    head_cords = [head[0].item(),head[1]]\n",
    "    snake.set_apple_cords(apple)\n",
    "    snake.set_head_cords(head_cords)\n",
    "    snake.set_neighbours()\n",
    "    if reward == -100:\n",
    "        break\n",
    "    path = astar( head_cords, apple, a)\n",
    "    if path == None:\n",
    "        break\n",
    "    cords = np.array(path)\n",
    "    # print(cords, np.array(head_cords))\n",
    "    cords-=np.array(head_cords)\n",
    "    if cords[0]==-31:\n",
    "        cords[0]=1\n",
    "    if cords[0]==31:\n",
    "        cords[0]=-1\n",
    "    if cords[1]==-31:\n",
    "        cords[1]=1\n",
    "    if cords[1]==31:\n",
    "        cords[1]=-1\n",
    "    # fig, ax = plt.subplots(1, 1)\n",
    "    # img = ax.imshow(a)\n",
    "    action = {'val': 1}\n",
    "    n = 0\n",
    "    score = None\n",
    "    # img.set_data(a)\n",
    "    cords_list=cords.tolist()\n",
    "    snake.set_dirrection(cords_list)\n",
    "    reward, score = do(snake.field, cords)\n",
    "    n += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7b41baea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [13, 9], [23, 1], [[12, 9], [13, 8], [13, 10], [14, 9]], [False, True, False, False]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcfUlEQVR4nO3df2yV9f338VdBekBpTy2lPe1oWQEFFdplndQTlaF0lO6OAakL/khWnMHAihl0Tu3m722pX0wUNRVyZxvMRERZBKL3xGmxJW6Fjc4Gf8yGkm7U0JZJ0nOg2NK0n/uPxbPvUSqe9py+e06fj+RK6DkX57wvrtanV8/pp0nOOScAAEbZBOsBAADjEwECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmLrIe4IsGBwd14sQJpaSkKCkpyXocAECEnHM6ffq0cnJyNGHC0Nc5Yy5AJ06cUG5urvUYAIARam9v14wZM4a8P2YBqq2t1ZNPPqnOzk4VFhbqueee08KFCy/491JSUiRJV97xkCYmT47VeBhFk/7Pp9YjxL3+/5dhPQLwtQ2c69VHL/4y9N/zocQkQC+//LKqqqq0detWFRcXa/PmzSotLVVLS4syMzO/8u9+/m23icmTCVCCmHiJx3qEuDfI1wLi0IVeRonJmxCeeuoprVmzRnfeeaeuvPJKbd26VRdffLF+97vfxeLpAABxKOoBOnfunJqamlRSUvLfJ5kwQSUlJWpsbPzS/n19fQoGg2EbACDxRT1An376qQYGBpSVlRV2e1ZWljo7O7+0f01Njbxeb2jjDQgAMD6Y/xxQdXW1AoFAaGtvb7ceCQAwCqL+JoSMjAxNnDhRXV1dYbd3dXXJ5/N9aX+PxyOPhxepAWC8ifoVUHJysoqKilRXVxe6bXBwUHV1dfL7/dF+OgBAnIrJ27CrqqpUUVGh73znO1q4cKE2b96snp4e3XnnnbF4OgBAHIpJgFatWqV///vfevjhh9XZ2alvfetb2rdv35femAAAGL+SnHPOeoj/LRgMyuv1asGdv+YHUceoSSv+HbPH7t8zPWaPPZbwb4hENnCuV+9v+4UCgYBSU1OH3M/8XXAAgPGJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEzEZC04xJdYLgsjsTTM+UT6bxLJOYr0fHJ+YIUrIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZYCy5BxXJ9N9YOG32R/JtHeu4j2Z9zj2jiCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATLAUT5xgaR18XZGez0g+tyL9PORzC1+FKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmWAvOEOu7YSyI5HMl0s/ZSPbnc3b84QoIAGAi6gF69NFHlZSUFLbNmzcv2k8DAIhzMfkW3FVXXaW33377v09yEd/pAwCEi0kZLrroIvl8vlg8NAAgQcTkNaCjR48qJydHs2bN0h133KHjx48PuW9fX5+CwWDYBgBIfFEPUHFxsbZv3659+/Zpy5Ytamtr0/XXX6/Tp0+fd/+amhp5vd7QlpubG+2RAABjUNQDVFZWph/84AcqKChQaWmp/vjHP6q7u1uvvPLKefevrq5WIBAIbe3t7dEeCQAwBsX83QFpaWm6/PLL1draet77PR6PPB5PrMcAAIwxMf85oDNnzujYsWPKzs6O9VMBAOJI1AN07733qqGhQf/85z/1l7/8RTfffLMmTpyo2267LdpPBQCIY1H/Ftwnn3yi2267TadOndL06dN13XXX6eDBg5o+PfGX2WBpHSS6SD8PI/maiPTrh6+J+Bf1AO3cuTPaDwkASECsBQcAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJmL+6xjiHeu7AcMXyed4pF9rkezP19rYxBUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhgKZ4oYrkPYPgi/fqJZCmeSJf54Wt5dHAFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwARrwQGIS5Gs1xbpWnCsHTc6uAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggrXgACS8SNdqi+Xacawb919cAQEATEQcoAMHDuimm25STk6OkpKStGfPnrD7nXN6+OGHlZ2drSlTpqikpERHjx6N1rwAgAQRcYB6enpUWFio2tra896/adMmPfvss9q6dasOHTqkSy65RKWlpert7R3xsACAxBHxa0BlZWUqKys7733OOW3evFkPPvigli9fLkl64YUXlJWVpT179ujWW28d2bQAgIQR1deA2tra1NnZqZKSktBtXq9XxcXFamxsPO/f6evrUzAYDNsAAIkvqgHq7OyUJGVlZYXdnpWVFbrvi2pqauT1ekNbbm5uNEcCAIxR5u+Cq66uViAQCG3t7e3WIwEARkFUA+Tz+SRJXV1dYbd3dXWF7vsij8ej1NTUsA0AkPiiGqD8/Hz5fD7V1dWFbgsGgzp06JD8fn80nwoAEOcifhfcmTNn1NraGvq4ra1Nzc3NSk9PV15enjZs2KBf/epXuuyyy5Sfn6+HHnpIOTk5WrFiRTTnBgDEuYgDdPjwYd1www2hj6uqqiRJFRUV2r59u+677z719PTo7rvvVnd3t6677jrt27dPkydPjt7UABBDsVy6J9JlfhJ56Z6IA7R48WI554a8PykpSY8//rgef/zxEQ0GAEhs5u+CAwCMTwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgIuKleAAA4SJZry3SteAi2T/e1o3jCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATLAUDyJeGuTgt/4Q0f5Fj66LaP9YifQ4AcQWV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMsBZcgorlumdjZW03KbbH2b9nesweGwBXQAAAIwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABggqV44sR4WXJmvBwnAK6AAABGCBAAwETEATpw4IBuuukm5eTkKCkpSXv27Am7f/Xq1UpKSgrbli1bFq15AQAJIuIA9fT0qLCwULW1tUPus2zZMnV0dIS2l156aURDAgAST8RvQigrK1NZWdlX7uPxeOTz+YY9FAAg8cXkNaD6+nplZmZq7ty5WrdunU6dOjXkvn19fQoGg2EbACDxRT1Ay5Yt0wsvvKC6ujr9z//8jxoaGlRWVqaBgYHz7l9TUyOv1xvacnNzoz0SAGAMivrPAd16662hPy9YsEAFBQWaPXu26uvrtWTJki/tX11draqqqtDHwWCQCAHAOBDzt2HPmjVLGRkZam1tPe/9Ho9HqampYRsAIPHFPECffPKJTp06pezs7Fg/FQAgjkT8LbgzZ86EXc20tbWpublZ6enpSk9P12OPPaby8nL5fD4dO3ZM9913n+bMmaPS0tKoDg4AiG8RB+jw4cO64YYbQh9//vpNRUWFtmzZoiNHjuj3v/+9uru7lZOTo6VLl+qXv/ylPB5P9KZOEONl3bPxcpwAIhNxgBYvXizn3JD3v/nmmyMaCAAwPrAWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYiPrvAxrPxsuaZ+PlOAHEFldAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCpXgMjZVlZ2K5tI40do4TwNjCFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATrAWXoGK5vhtruwGIBq6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAES/HECZbWAZBouAICAJiIKEA1NTW6+uqrlZKSoszMTK1YsUItLS1h+/T29qqyslLTpk3T1KlTVV5erq6urqgODQCIfxEFqKGhQZWVlTp48KDeeust9ff3a+nSperp6Qnts3HjRr322mvatWuXGhoadOLECa1cuTLqgwMA4ltErwHt27cv7OPt27crMzNTTU1NWrRokQKBgH77299qx44duvHGGyVJ27Zt0xVXXKGDBw/qmmuuid7kAIC4NqLXgAKBgCQpPT1dktTU1KT+/n6VlJSE9pk3b57y8vLU2Nh43sfo6+tTMBgM2wAAiW/YARocHNSGDRt07bXXav78+ZKkzs5OJScnKy0tLWzfrKwsdXZ2nvdxampq5PV6Q1tubu5wRwIAxJFhB6iyslIffPCBdu7cOaIBqqurFQgEQlt7e/uIHg8AEB+G9XNA69ev1+uvv64DBw5oxowZodt9Pp/OnTun7u7usKugrq4u+Xy+8z6Wx+ORx+MZzhgAgDgW0RWQc07r16/X7t27tX//fuXn54fdX1RUpEmTJqmuri50W0tLi44fPy6/3x+diQEACSGiK6DKykrt2LFDe/fuVUpKSuh1Ha/XqylTpsjr9equu+5SVVWV0tPTlZqaqnvuuUd+v593wAEAwkQUoC1btkiSFi9eHHb7tm3btHr1aknS008/rQkTJqi8vFx9fX0qLS3V888/H5VhAQCJI6IAOecuuM/kyZNVW1ur2traYQ81XrC+G4DxjLXgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMDEsH4dA0YfS+sASDRcAQEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADCREGvBZfzfxq+976d3+2M2B+u1AcDXxxUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJhIiKV4Yrm8DgAgNrgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCIh1oKLpf49061HAICExBUQAMBERAGqqanR1VdfrZSUFGVmZmrFihVqaWkJ22fx4sVKSkoK29auXRvVoQEA8S+iADU0NKiyslIHDx7UW2+9pf7+fi1dulQ9PT1h+61Zs0YdHR2hbdOmTVEdGgAQ/yJ6DWjfvn1hH2/fvl2ZmZlqamrSokWLQrdffPHF8vl80ZkQAJCQRvQaUCAQkCSlp6eH3f7iiy8qIyND8+fPV3V1tc6ePTvkY/T19SkYDIZtAIDEN+x3wQ0ODmrDhg269tprNX/+/NDtt99+u2bOnKmcnBwdOXJE999/v1paWvTqq6+e93Fqamr02GOPDXcMAECcSnLOueH8xXXr1umNN97Qu+++qxkzZgy53/79+7VkyRK1trZq9uzZX7q/r69PfX19oY+DwaByc3O14M5fa2Ly5OGMBgAwNHCuV+9v+4UCgYBSU1OH3G9YV0Dr16/X66+/rgMHDnxlfCSpuLhYkoYMkMfjkcfjGc4YAIA4FlGAnHO65557tHv3btXX1ys/P/+Cf6e5uVmSlJ2dPawBAQCJKaIAVVZWaseOHdq7d69SUlLU2dkpSfJ6vZoyZYqOHTumHTt26Pvf/76mTZumI0eOaOPGjVq0aJEKCgpicgAAgPgUUYC2bNki6T8/bPq/bdu2TatXr1ZycrLefvttbd68WT09PcrNzVV5ebkefPDBqA0MAEgMEX8L7qvk5uaqoaFhRAMBAMYH1oIDAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYiCtCWLVtUUFCg1NRUpaamyu/364033gjd39vbq8rKSk2bNk1Tp05VeXm5urq6oj40ACD+RRSgGTNm6IknnlBTU5MOHz6sG2+8UcuXL9eHH34oSdq4caNee+017dq1Sw0NDTpx4oRWrlwZk8EBAPEtyTnnRvIA6enpevLJJ3XLLbdo+vTp2rFjh2655RZJ0scff6wrrrhCjY2Nuuaaa77W4wWDQXm9Xi2489eamDx5JKMBAAwMnOvV+9t+oUAgoNTU1CH3G/ZrQAMDA9q5c6d6enrk9/vV1NSk/v5+lZSUhPaZN2+e8vLy1NjYOOTj9PX1KRgMhm0AgMQXcYDef/99TZ06VR6PR2vXrtXu3bt15ZVXqrOzU8nJyUpLSwvbPysrS52dnUM+Xk1Njbxeb2jLzc2N+CAAAPEn4gDNnTtXzc3NOnTokNatW6eKigp99NFHwx6gurpagUAgtLW3tw/7sQAA8eOiSP9CcnKy5syZI0kqKirS3/72Nz3zzDNatWqVzp07p+7u7rCroK6uLvl8viEfz+PxyOPxRD45ACCujfjngAYHB9XX16eioiJNmjRJdXV1oftaWlp0/Phx+f3+kT4NACDBRHQFVF1drbKyMuXl5en06dPasWOH6uvr9eabb8rr9equu+5SVVWV0tPTlZqaqnvuuUd+v/9rvwMOADB+RBSgkydP6oc//KE6Ojrk9XpVUFCgN998U9/73vckSU8//bQmTJig8vJy9fX1qbS0VM8//3xMBgcAxLcR/xxQtPFzQAAQ32L+c0AAAIwEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARMSrYcfa5wszDJzrNZ4EADAcn//3+0IL7Yy5pXg++eQTfikdACSA9vZ2zZgxY8j7x1yABgcHdeLECaWkpCgpKSl0ezAYVG5urtrb279ybaF4x3EmjvFwjBLHmWiicZzOOZ0+fVo5OTmaMGHoV3rG3LfgJkyY8JXFTE1NTeiT/zmOM3GMh2OUOM5EM9Lj9Hq9F9yHNyEAAEwQIACAibgJkMfj0SOPPCKPx2M9SkxxnIljPByjxHEmmtE8zjH3JgQAwPgQN1dAAIDEQoAAACYIEADABAECAJiImwDV1tbqm9/8piZPnqzi4mL99a9/tR4pqh599FElJSWFbfPmzbMea0QOHDigm266STk5OUpKStKePXvC7nfO6eGHH1Z2dramTJmikpISHT161GbYEbjQca5evfpL53bZsmU2ww5TTU2Nrr76aqWkpCgzM1MrVqxQS0tL2D69vb2qrKzUtGnTNHXqVJWXl6urq8to4uH5Ose5ePHiL53PtWvXGk08PFu2bFFBQUHoh039fr/eeOON0P2jdS7jIkAvv/yyqqqq9Mgjj+jvf/+7CgsLVVpaqpMnT1qPFlVXXXWVOjo6Qtu7775rPdKI9PT0qLCwULW1tee9f9OmTXr22We1detWHTp0SJdccolKS0vV2xtfC9Fe6DgladmyZWHn9qWXXhrFCUeuoaFBlZWVOnjwoN566y319/dr6dKl6unpCe2zceNGvfbaa9q1a5caGhp04sQJrVy50nDqyH2d45SkNWvWhJ3PTZs2GU08PDNmzNATTzyhpqYmHT58WDfeeKOWL1+uDz/8UNIonksXBxYuXOgqKytDHw8MDLicnBxXU1NjOFV0PfLII66wsNB6jJiR5Hbv3h36eHBw0Pl8Pvfkk0+Gbuvu7nYej8e99NJLBhNGxxeP0znnKioq3PLly03miZWTJ086Sa6hocE5959zN2nSJLdr167QPv/4xz+cJNfY2Gg15oh98Tidc+673/2u+8lPfmI3VIxceuml7je/+c2onssxfwV07tw5NTU1qaSkJHTbhAkTVFJSosbGRsPJou/o0aPKycnRrFmzdMcdd+j48ePWI8VMW1ubOjs7w86r1+tVcXFxwp1XSaqvr1dmZqbmzp2rdevW6dSpU9YjjUggEJAkpaenS5KamprU398fdj7nzZunvLy8uD6fXzzOz7344ovKyMjQ/PnzVV1drbNnz1qMFxUDAwPauXOnenp65Pf7R/VcjrnFSL/o008/1cDAgLKyssJuz8rK0scff2w0VfQVFxdr+/btmjt3rjo6OvTYY4/p+uuv1wcffKCUlBTr8aKus7NTks57Xj+/L1EsW7ZMK1euVH5+vo4dO6af//znKisrU2NjoyZOnGg9XsQGBwe1YcMGXXvttZo/f76k/5zP5ORkpaWlhe0bz+fzfMcpSbfffrtmzpypnJwcHTlyRPfff79aWlr06quvGk4buffff19+v1+9vb2aOnWqdu/erSuvvFLNzc2jdi7HfIDGi7KystCfCwoKVFxcrJkzZ+qVV17RXXfdZTgZRurWW28N/XnBggUqKCjQ7NmzVV9fryVLlhhONjyVlZX64IMP4v41ygsZ6jjvvvvu0J8XLFig7OxsLVmyRMeOHdPs2bNHe8xhmzt3rpqbmxUIBPSHP/xBFRUVamhoGNUZxvy34DIyMjRx4sQvvQOjq6tLPp/PaKrYS0tL0+WXX67W1lbrUWLi83M33s6rJM2aNUsZGRlxeW7Xr1+v119/Xe+8807Yr03x+Xw6d+6curu7w/aP1/M51HGeT3FxsSTF3flMTk7WnDlzVFRUpJqaGhUWFuqZZ54Z1XM55gOUnJysoqIi1dXVhW4bHBxUXV2d/H6/4WSxdebMGR07dkzZ2dnWo8REfn6+fD5f2HkNBoM6dOhQQp9X6T+/9ffUqVNxdW6dc1q/fr12796t/fv3Kz8/P+z+oqIiTZo0Kex8trS06Pjx43F1Pi90nOfT3NwsSXF1Ps9ncHBQfX19o3suo/qWhhjZuXOn83g8bvv27e6jjz5yd999t0tLS3OdnZ3Wo0XNT3/6U1dfX+/a2trcn//8Z1dSUuIyMjLcyZMnrUcbttOnT7v33nvPvffee06Se+qpp9x7773n/vWvfznnnHviiSdcWlqa27t3rzty5Ihbvny5y8/Pd5999pnx5JH5quM8ffq0u/fee11jY6Nra2tzb7/9tvv2t7/tLrvsMtfb22s9+te2bt065/V6XX19vevo6AhtZ8+eDe2zdu1al5eX5/bv3+8OHz7s/H6/8/v9hlNH7kLH2dra6h5//HF3+PBh19bW5vbu3etmzZrlFi1aZDx5ZB544AHX0NDg2tra3JEjR9wDDzzgkpKS3J/+9Cfn3Oidy7gIkHPOPffccy4vL88lJye7hQsXuoMHD1qPFFWrVq1y2dnZLjk52X3jG99wq1atcq2trdZjjcg777zjJH1pq6iocM79563YDz30kMvKynIej8ctWbLEtbS02A49DF91nGfPnnVLly5106dPd5MmTXIzZ850a9asibv/eTrf8Uly27ZtC+3z2WefuR//+Mfu0ksvdRdffLG7+eabXUdHh93Qw3Ch4zx+/LhbtGiRS09Pdx6Px82ZM8f97Gc/c4FAwHbwCP3oRz9yM2fOdMnJyW769OluyZIlofg4N3rnkl/HAAAwMeZfAwIAJCYCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMT/B+ihc+EoDZCjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "a = torch.zeros(snake.field.shape)\n",
    "positions = snake.field.flatten().topk(2)[1]\n",
    "[pos_cur, pos_prev] = [torch.Tensor(unravel(x, snake.field.shape)) for x in positions]\n",
    "a[snake.field>0]=1\n",
    "a[snake.field==snake.field.max()]=2\n",
    "a[snake.field<0]=-1\n",
    "# print(a)\n",
    "img = ax.imshow(a)\n",
    "action = {'val': 1}\n",
    "# print(a)\n",
    "n = 0\n",
    "score = None\n",
    "\n",
    "while n<1:\n",
    "    img.set_data(a)\n",
    "    # reward, score = do(snake.field, action_dict['a'])\n",
    "    n += 1\n",
    "# print(reward, score)\n",
    "# print(score)\n",
    "state = snake.get_state()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d3b2be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_move = [0,0,0]\n",
    "if random.randint(0, 200) < 80:\n",
    "    move = random.randint(0, 2)\n",
    "    final_move[move] = 1\n",
    "np.argmax(final_move)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
